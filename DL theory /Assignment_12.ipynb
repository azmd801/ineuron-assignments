{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory%20/Assignment_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answers to the Provided Questions\n",
        "\n",
        "### 1. How does unsqueeze help us to solve certain broadcasting problems?\n",
        "The `unsqueeze` operation in PyTorch helps in adding a dimension of size one at a specified position. This is often used to solve broadcasting problems, especially when we want to perform operations between tensors of different dimensions by making their dimensions compatible.\n",
        "\n",
        "### 2. How can we use indexing to do the same operation as unsqueeze?\n",
        "We can use indexing to add an extra dimension to a tensor, similar to what `unsqueeze` does. This can be achieved by using `None` or `numpy.newaxis` in the indexing operation. Let's demonstrate this with a code example:\n",
        "\n",
        "```\n",
        "# Example of using indexing to do the same operation as unsqueeze\n",
        "import torch\n",
        "\n",
        "# Creating a tensor of size (3,)\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "\n",
        "# Using indexing to add an extra dimension\n",
        "tensor_unsqueezed = tensor[None, :]\n",
        "tensor_unsqueezed\n",
        "```\n",
        "### 3. How do we show the actual contents of the memory used for a tensor?\n",
        "To show the actual contents of the memory used by a tensor, we can use the `.numpy()` function to convert the tensor to a numpy array and then print it. We will demonstrate this in the following code cell:\n",
        "\n",
        "\\```python\n",
        "# Showing the actual contents of the memory used for a tensor\n",
        "tensor_numpy = tensor.numpy()\n",
        "tensor_numpy\n",
        "\\```\n",
        "\n",
        "### 4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix?\n",
        "We will demonstrate adding a vector of size 3 to a matrix of size 3×3 through a Python code cell and observe whether the elements of the vector are added to each row or each column of the matrix.\n",
        "\n",
        "\\```python\n",
        "# Adding a vector of size 3 to a matrix of size 3x3\n",
        "import numpy as np\n",
        "\n",
        "# Creating a matrix of size 3x3\n",
        "matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Creating a vector of size 3\n",
        "vector = np.array([1, 2, 3])\n",
        "\n",
        "# Adding the vector to the matrix\n",
        "result = matrix + vector\n",
        "result\n",
        "\\```\n",
        "\n",
        "### 5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
        "Broadcasting and `expand_as` operations in PyTorch are designed to avoid unnecessary memory allocations. Broadcasting allows operations to be performed as if the smaller tensor has been expanded to match the larger tensor's size, but without actually allocating new memory for the expanded tensor. Similarly, `expand_as` returns a view of the original tensor with expanded size, without allocating new memory.\n",
        "\n",
        "### 6. Implement matmul using Einstein summation.\n",
        "We can implement matrix multiplication using the `torch.einsum` function, which allows specifying operations based on Einstein summation notation. We will demonstrate this in the following code cell:\n",
        "\n",
        "\\```python\n",
        "# Implementing matmul using Einstein summation\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "b = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# Performing matrix multiplication using einsum\n",
        "result = torch.einsum('ij,jk->ik', a, b)\n",
        "result\n",
        "\\```\n",
        "\n",
        "### 7. What does a repeated index letter represent on the lefthand side of einsum?\n",
        "In the `einsum` function, a repeated index letter on the left-hand side represents the dimensions over which the summation is performed. The repeated indices imply that the elements along these dimensions are multiplied and summed over to produce the output.\n",
        "\n",
        "### 8. What are the three rules of Einstein summation notation? Why?\n",
        "The Einstein summation notation follows three rules:\n",
        "1. **Repetition Rule**: An index that is repeated in a term is summed over.\n",
        "2. **Free Index Rule**: An index that appears only once in a term is a free index, and represents the dimensions of the output.\n",
        "3. **Dummy Index Rule**: A repeated index is known as a dummy index, and its name has no significance. It can be replaced with another letter without changing the meaning of the expression.\n",
        "\n",
        "These rules are important as they allow for concise expression of complex tensor operations, making the equations easier to read and write.\n",
        "\n",
        "### 9. What are the forward pass and backward pass of a neural network?\n",
        "In the context of neural networks, the **forward pass** refers to the process of passing the input data through the network, layer by layer, until the output layer is reached. The **backward pass**, also known as backpropagation, refers to the process of computing gradients of the loss function with respect to the weights of the network, which are then used to update the weights to minimize the loss function. The gradients are computed using the chain rule of calculus, working backwards from the output layer to the input layer.\n",
        "\n",
        "### 10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
        "Storing some of the activations calculated for intermediate layers during the forward pass is necessary for the backward pass (backpropagation) where gradients are computed for updating the weights. These stored activations are used in the computation of gradients, as the gradients at each layer are computed with respect to both the weights and activations of that layer. Without storing these activations, we would not be able to compute the gradients during the backward pass, or it would require recomputing them, which is inefficient.\n",
        "\n",
        "### 11. What is the downside of having activations with a standard deviation too far away from 1?\n",
        "Having activations with a standard deviation too far away from 1 can lead to problems such as gradient vanishing or exploding, which can hinder the learning process. When the standard deviation is too small, the gradients can become too small to make significant updates (gradient vanishing). Conversely, when the standard deviation is too large, the gradients can become too large, leading to unstable updates (gradient exploding). Both cases can make it difficult for the network to learn effectively, and can result in a model that does not converge or converges to a suboptimal solution.\n",
        "\n",
        "### 12. How can weight initialization help avoid this problem?\n",
        "Proper weight initialization can help maintain the activations at a desirable scale, avoiding the issues of gradient vanishing or exploding. Initializing the weights such that the activations have a standard deviation close to 1 can promote stable and faster convergence during training. Different initialization strategies, such as Xavier/Glorot initialization or He initialization, are designed to keep the standard deviation of activations within a reasonable range, based on the activation function used in the network layers.\n"
      ],
      "metadata": {
        "id": "gdHAwxHwWFPb"
      },
      "id": "gdHAwxHwWFPb"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zM0qLvzl9GL"
      },
      "id": "2zM0qLvzl9GL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}