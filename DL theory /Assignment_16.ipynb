{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory%20/Assignment_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Explain the Activation Functions in your own language\n",
        "#### a) Sigmoid\n",
        "**Answer:** The sigmoid function squishes input values to be between 0 and 1, making it useful for binary classification problems. It resembles an \"S\" shaped curve.\n",
        "\n",
        "#### b) Tanh\n",
        "**Answer:** Tanh, or hyperbolic tangent, is like a scaled version of the sigmoid. It squishes values to be between -1 and 1, thus centering the output around 0, which can make learning for the next layer easier.\n",
        "\n",
        "#### c) ReLU\n",
        "**Answer:** ReLU, or Rectified Linear Unit, is a function that outputs the input directly if it is positive; otherwise, it will output zero. It has become popular because it reduces the likelihood of the vanishing gradient problem and is computationally efficient.\n",
        "\n",
        "#### d) ELU\n",
        "**Answer:** ELU, or Exponential Linear Unit, is like ReLU, but it allows a small amount of negative values for negative inputs, which can help the network learn more robustly.\n",
        "\n",
        "#### e) LeakyReLU\n",
        "**Answer:** LeakyReLU is a variant of ReLU that has a small positive slope (or \"leak\") for negative input values, preventing neurons from \"dying\" during training.\n",
        "\n",
        "#### f) Swish\n",
        "**Answer:** Swish is a newer activation function that tends to outperform ReLU in deeper networks. It is defined as \\( f(x) = x \\cdot \\text{sigmoid}(\\beta x) \\), where \\( \\beta \\) is a trainable parameter.\n",
        "\n",
        "### 2. What happens when you increase or decrease the optimizer learning rate?\n",
        "\n",
        "**Answer:** Increasing the learning rate can make the training faster but it might overshoot the optimal solution. On the other hand, decreasing the learning rate can make the training more stable but too small a value might result in a very slow convergence or getting stuck in local minima.\n",
        "\n",
        "### 3. What happens when you increase the number of internal hidden neurons?\n",
        "\n",
        "**Answer:** Increasing the number of hidden neurons can potentially increase the model's capacity to learn from the data by capturing more complex patterns. However, it might also increase the risk of overfitting to the training data, especially if the number of neurons is unnecessarily large.\n",
        "\n",
        "### 4. What happens when you increase the size of batch computation?\n",
        "\n",
        "**Answer:** Increasing the batch size can make the training faster as it performs updates less frequently. However, it might require more memory and might result in a less stable training process with more fluctuations in the learning curve.\n",
        "\n",
        "### 5. Why we adopt regularization to avoid overfitting?\n",
        "\n",
        "**Answer:** Regularization adds a penalty on the larger weights to prevent the model from fitting too closely to the noise in the training data. This helps in generalizing better to new, unseen data by avoiding overfitting.\n",
        "\n",
        "### 6. What are loss and cost functions in deep learning?\n",
        "\n",
        "**Answer:** Loss functions compute the error for a single training example, while the cost function is the average of the loss functions of the entire training set. These functions are used to train the model by minimizing them through optimization algorithms like gradient descent.\n",
        "\n",
        "### 7. What do you mean by underfitting in neural networks?\n",
        "\n",
        "**Answer:** Underfitting refers to a model that cannot capture the underlying trend of the data. This happens when the model is too simple to handle the complexity of the data. It performs poorly both on the training data and unseen data.\n",
        "\n",
        "### 8. Why we use Dropout in Neural Networks?\n",
        "\n",
        "**Answer:** Dropout is a regularization technique that helps to prevent overfitting in neural networks. During training, it randomly \"drops out\" or deactivates a fraction of neurons in the layer, forcing the network to learn more robust and generalized features, as it cannot rely on any one neuron being present during the predictions."
      ],
      "metadata": {
        "id": "rn0EhN0Qx7Sb"
      },
      "id": "rn0EhN0Qx7Sb"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7E2xNOP5x8j9"
      },
      "id": "7E2xNOP5x8j9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}