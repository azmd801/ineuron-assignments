{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAn33M25V8m6zgFJ4fRqrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory%20/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
        "\n",
        "   No, it's not recommended to initialize all the weights to the same value even with He initialization. Initializing all weights to the same value would lead to symmetry in the network, causing all neurons to update the same way during training. This would hinder the network's ability to learn and break the capacity to model complex relationships.\n",
        "\n",
        "2. **Is it OK to initialize the bias terms to 0?**\n",
        "\n",
        "   Yes, it is generally fine to initialize bias terms to 0. Biases control the output when all inputs are 0, and setting them to 0 does not introduce symmetry issues as weight initialization does.\n",
        "\n",
        "3. **Name three advantages of the SELU activation function over ReLU.**\n",
        "\n",
        "   The Scaled Exponential Linear Unit (SELU) has several advantages over ReLU:\n",
        "   \n",
        "   - **Self-Normalization:** SELU activations maintain a mean close to 0 and standard deviation close to 1, aiding training stability.\n",
        "   - **Vanishing Gradient Mitigation:** SELU mitigates vanishing gradient issues, enabling deeper networks.\n",
        "   - **Non-Zero Gradients for Negative Inputs:** Unlike ReLU, SELU has non-zero gradients for negative inputs, preventing dead neurons.\n",
        "\n",
        "4. **In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
        "\n",
        "   - **SELU:** Ideal for deep networks due to self-normalization and vanishing gradient prevention. Use when training deep architectures.\n",
        "   - **Leaky ReLU:** Use when addressing dying ReLU problem. Variants like Parametric ReLU (PReLU) adaptively learn leaky coefficients.\n",
        "   - **ReLU:** Generally a good default choice. Efficient and prevents vanishing gradients. Not suitable for outputs that need to be strictly positive.\n",
        "   - **Tanh:** Suited for hidden layers. Maps inputs to the range [-1, 1], aiding in avoiding vanishing gradients for deeper architectures.\n",
        "   - **Logistic (Sigmoid):** Suitable for binary classification problems where outputs need to be between 0 and 1.\n",
        "   - **Softmax:** Used in the output layer for multi-class classification to produce probability distributions over classes.\n",
        "\n",
        "5. **What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?**\n",
        "\n",
        "   Setting the momentum hyperparameter too close to 1 in Stochastic Gradient Descent (SGD) can lead to slow convergence or even divergence. The momentum term accumulates a large portion of the previous gradients, causing the optimization process to overshoot and miss the minimum of the loss function.\n",
        "\n",
        "6. **Name three ways you can produce a sparse model.**\n",
        "\n",
        "   - **L1 Regularization (Lasso):** The L1 regularization term encourages weights to become exactly 0, leading to sparse models.\n",
        "   - **Dropout:** Randomly deactivating neurons during training prevents co-adaptation of specific neurons and encourages learning from others.\n",
        "   - **Sparse Activations:** Encourage neurons to activate only for specific inputs, leading to sparse activations.\n",
        "\n",
        "7. **Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**\n",
        "\n",
        "   - **Dropout and Training:** Yes, dropout can slow down training since fewer neurons are used during each training iteration, requiring more iterations to converge.\n",
        "   - **Dropout and Inference:** No, dropout is not applied during inference, so it doesn't slow down predictions on new instances.\n",
        "   - **MC Dropout:** Monte Carlo Dropout involves using dropout at inference time with multiple runs and averaging predictions. It can slow down inference due to multiple runs."
      ],
      "metadata": {
        "id": "a68tp4CEGy95"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2KZ-7qwG1Bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}