{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfsXz+nBcRW8zv2lBJ6O6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory%20/Assignment_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
        "\n",
        "#### Pros of using a Stateful RNN:\n",
        "- Maintains information about the sequence across different batches, which can potentially capture patterns over long sequences.\n",
        "- Can potentially learn deeper patterns in the data as it maintains the state between batches.\n",
        "\n",
        "#### Cons of using a Stateful RNN:\n",
        "- More complexity in managing the states between the sequences, which can lead to difficulties in training.\n",
        "- Higher computational cost and memory usage compared to stateless RNNs.\n",
        "- The sequences need to be non-overlapping during training.\n",
        "\n",
        "### 2. Why do people use Encoderâ€“Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
        "\n",
        "Encoder-Decoder RNNs are better suited for tasks like translation as they first encode the entire input sequence (source language) into a fixed-length context vector, and then decode it to produce the output sequence (target language). This helps in capturing the context of the entire sequence, which is essential in translation tasks. Plain sequence-to-sequence RNNs might not be able to capture the necessary context when translating complex sentences.\n",
        "\n",
        "### 3. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
        "\n",
        "To handle variable-length input sequences, padding can be used to make all sequences the same length while training the model. For variable-length output sequences, techniques like using a special token to indicate the end of the sequence or using dynamic computation graphs can be employed.\n",
        "\n",
        "### 4. What is beam search and why would you use it? What tool can you use to implement it?\n",
        "\n",
        "Beam search is a heuristic search algorithm that explores the most promising nodes in a graph, unlike greedy search which only picks the best option at each step. Beam search keeps track of `k` best options at each step which helps in finding a more optimal solution in sequence-to-sequence tasks. It can be implemented using libraries like TensorFlow and PyTorch.\n",
        "\n",
        "### 5. What is an attention mechanism? How does it help?\n",
        "\n",
        "The attention mechanism helps the model to focus on different parts of the input sequence when producing an output sequence, essentially allowing the model to \"attend\" to different input parts differently. It has proven to be very effective in tasks like machine translation, where it helps in aligning the words in the source and target languages correctly.\n",
        "\n",
        "### 6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
        "\n",
        "The most important layer in the Transformer architecture is the Attention layer. Its purpose is to model the dependencies between different words in a sequence irrespective of their positions, thus enabling parallel computation and capturing long-range dependencies in the data more effectively compared to RNNs.\n",
        "\n",
        "### 7. When would you need to use sampled softmax?\n",
        "\n",
        "Sampled softmax is generally used in scenarios where the output vocabulary size is extremely large, making the computation of the softmax function computationally expensive. In such cases, sampled softmax approximates the softmax function by considering only a subset of the entire vocabulary, thus making the training process faster and more scalable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tGgkJPdA1MpU"
      }
    }
  ]
}