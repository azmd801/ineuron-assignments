{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSYay+0EdNW5RdJhnCNokW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory%20/Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command will show the entire contents of the SavedModel directory including MetaGraphDefs, tags, and SignatureDefs.\n",
        "\n",
        "### 2. When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
        "\n",
        "You should use TF Serving when you need to deploy TensorFlow models for production environments. Its main features include:\n",
        "\n",
        "1. Easy deployment of TensorFlow models.\n",
        "2. High-performance serving with support for hardware accelerations.\n",
        "3. Support for both RESTful and gRPC APIs.\n",
        "4. Can seamlessly integrate with Docker and Kubernetes for scalable deployments.\n",
        "\n",
        "For deploying it, you can use tools such as Docker, Kubernetes, or use native installations.\n",
        "\n",
        "### 3. How do you deploy a model across multiple TF Serving instances?\n",
        "\n",
        "To deploy a model across multiple TF Serving instances, you generally follow these steps:\n",
        "\n",
        "1. Containerize the TF Serving instance using a tool like Docker.\n",
        "2. Deploy the containers to a container orchestration platform like Kubernetes.\n",
        "3. Configure the platform to manage and scale the deployment, ensuring load balancing and redundancy across the instances.\n",
        "\n",
        "### 4. When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
        "\n",
        "You should use the gRPC API when you require more efficiency and lower latency, as it utilizes protocol buffers which are more efficient than JSON used in REST APIs. It is also beneficial when you have a client and server written in different languages as gRPC supports multiple programming languages.\n",
        "\n",
        "### 5. What are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile or embedded device?\n",
        "\n",
        "TFLite reduces a model's size through various methods including:\n",
        "\n",
        "1. Quantization: Reduces the precision of the model's weights.\n",
        "2. Pruning: Removes unnecessary parameters from the model.\n",
        "3. Using optimized kernels: Utilizes kernels that are optimized for mobile and embedded devices.\n",
        "4. Model Simplification: Simplifies the model architecture to remove complex operations.\n",
        "\n",
        "### 6. What is quantization-aware training, and why would you need it?\n",
        "\n",
        "Quantization-aware training is a technique where the model is trained to be aware of the lower precision computations that will be used during inference. This is necessary because it allows the model to maintain a high level of accuracy even when its weights and activations are quantized to lower precision, which helps in reducing the model size and speeding up the inference time.\n",
        "\n",
        "### 7. What are model parallelism and data parallelism? Why is the latter generally recommended?\n",
        "\n",
        "Model parallelism is the process where different parts of the neural network model are executed on different processors. Data parallelism, on the other hand, involves distributing different data batches across multiple processors and each processor executes the whole model. Data parallelism is generally recommended as it is simpler to implement and can effectively utilize the available computational resources.\n",
        "\n",
        "### 8. When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
        "\n",
        "When training a model across multiple servers, you can use distribution strategies like MirroredStrategy, ParameterServerStrategy, and MultiWorkerMirroredStrategy. The choice of strategy depends on various factors including the hardware setup, the complexity of the model, and the size of the dataset. Generally, you would choose a strategy that best aligns with your available resources and the specific requirements of your training workload."
      ],
      "metadata": {
        "id": "r8ZFAIvA3c8t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qw8P-W533wQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}