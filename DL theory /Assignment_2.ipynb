{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO18mPdZh23Oi0IFPyTvtOh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory%20/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?**\n",
        "\n",
        "   An artificial neuron, much like a biological neuron, consists of several components:\n",
        "\n",
        "   - **Inputs:** These are analogous to the dendrites of a biological neuron. Inputs convey signals from other neurons or sources.\n",
        "   - **Weights:** Similar to synapses, weights are assigned to inputs, indicating their relative importance.\n",
        "   - **Summation Function:** The weighted inputs are summed up, equivalent to the biological neuron's integration of incoming signals.\n",
        "   - **Activation Function:** This function determines the output of the neuron based on the aggregated sum. It models the firing threshold of a biological neuron.\n",
        "   - **Output:** The final output is produced by the activation function. This output can be passed to other neurons in subsequent layers.\n",
        "\n",
        "2. **What are the different types of activation functions popularly used? Explain each of them.**\n",
        "\n",
        "   There are several popular activation functions:\n",
        "\n",
        "   - **Step Function:** Outputs a binary value based on whether the input is above or below a threshold. It's simple but non-differentiable.\n",
        "   - **Sigmoid Function:** S-shaped curve that maps inputs to values between 0 and 1. It's smooth and allows gradient-based learning.\n",
        "   - **ReLU (Rectified Linear Unit):** Outputs the input if it's positive, otherwise outputs zero. It's widely used in deep learning for mitigating vanishing gradient issues.\n",
        "   - **Tanh (Hyperbolic Tangent):** Similar to the sigmoid but outputs values between -1 and 1. It's zero-centered and often used in hidden layers.\n",
        "   - **Softmax Function:** Used in multi-class classification to produce a probability distribution over classes.\n",
        "\n",
        "3. **Explain, in detail, Rosenblattâ€™s perceptron model. How can a set of data be classified using a simple perceptron?**\n",
        "\n",
        "   Rosenblatt's perceptron is a linear binary classifier. It takes input features, multiplies them by weights, sums them, and applies a step function. If the output is above a threshold, it's classified as one class; otherwise, as the other. A set of data can be classified by adjusting the weights based on misclassifications until convergence.\n",
        "\n",
        "\n",
        "\n",
        "5. **Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem.**\n",
        "\n",
        "   A multi-layer perceptron (MLP) consists of an input layer, one or more hidden layers, and an output layer. Hidden layers introduce non-linearity. An MLP can solve the XOR problem by learning non-linear decision boundaries through hidden layers. XOR is not linearly separable, but with the right weights and activation functions in hidden layers, an MLP can learn to distinguish XOR patterns.\n",
        "\n",
        "6. **What is an artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN.**\n",
        "\n",
        "   An artificial neural network (ANN) is a computational model inspired by the human brain. It consists of interconnected neurons organized into layers. Highlights of different ANN architectures include feedforward networks for data flow in one direction, recurrent networks with loops for memory, convolutional networks for image data, and more advanced architectures like transformers for sequence tasks.\n",
        "\n",
        "7. **Explain the learning process of an ANN. Explain, with an example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?**\n",
        "\n",
        "   ANN learning involves adjusting weights to minimize the difference between predicted and actual outputs. The challenge is assigning initial weights; incorrect weights lead to slow convergence or suboptimal results. Example: Assigning random weights may lead to a poor starting point. This challenge can be addressed by techniques like weight initialization methods (Xavier, He), which consider network architecture and activation functions.\n",
        "\n",
        "8. **Explain, in detail, the backpropagation algorithm. What are the limitations of this algorithm?**\n",
        "\n",
        "   Backpropagation is a supervised learning algorithm for training neural networks. It involves the following steps: forward pass to compute predicted outputs, calculation of error, backward pass to propagate error and update weights using gradient descent. Limitations include slow convergence in deep networks, sensitivity to initial weights, and challenges in handling vanishing and exploding gradients.\n",
        "\n",
        "9. **Describe, in detail, the process of adjusting the interconnection weights in a multi-layer neural network.**\n",
        "\n",
        "   Weight adjustment involves the backpropagation algorithm:\n",
        "   - **Forward Pass:** Compute predicted outputs using current weights.\n",
        "   - **Calculate Error:** Measure the difference between predicted and actual outputs.\n",
        "   - **Backward Pass:** Propagate error backward through the network, calculating gradients.\n",
        "   - **Update Weights:** Adjust weights using gradients and learning rate to minimize error.\n",
        "\n",
        "10. **What are the steps in the backpropagation algorithm? Why is a multi-layer neural network required?**\n",
        "\n",
        "    Backpropagation involves forward and backward passes. In the forward pass, inputs are processed layer by layer to produce outputs. In the backward pass, errors are propagated backward, and weights are adjusted using gradients to minimize errors. A multi-layer network is required to capture complex, non-linear relationships in data that single-layer networks cannot model.\n",
        "\n",
        "11. **Write short notes on:**\n",
        "\n",
        "    1. **Artificial Neuron:** An artificial neuron is a basic processing unit in a neural network. It receives inputs, applies weights and activation functions, and produces an output. It mimics the behavior of a biological neuron but in a mathematical and computational context.\n",
        "    \n",
        "    2. **Multi-layer Perceptron:** A multi-layer perceptron (MLP) is a type of neural network with an input layer, one or more hidden layers, and an output layer. It can model complex relationships between inputs and outputs by introducing non-linearity through hidden layers.\n",
        "    \n",
        "    3. **Deep Learning:** Deep learning refers to neural networks with many hidden layers, enabling the network to learn intricate patterns and features from data. It has achieved significant breakthroughs in tasks like image recognition, natural language processing, and more.\n",
        "    \n",
        "    4. **Learning Rate:** The learning rate determines the step size in weight updates during training. It affects how quickly the network adapts to the data and the convergence speed. A higher learning rate may cause overshooting, while a lower rate may lead to slow convergence.\n",
        "\n",
        "12. **Write the difference between:**\n",
        "\n",
        "    1. **Activation Function vs Threshold Function**\n",
        "       - **Activation Function:** It transforms the weighted sum of inputs to produce the output of a neuron. It introduces non-linearity, allowing neural networks to model complex relationships.\n",
        "       - **Threshold Function:** It produces binary outputs based on whether the input crosses a certain threshold. It's a simple step-like function that lacks the continuous and differentiable properties of activation functions.\n",
        "\n",
        "    2. **Step Function vs Sigmoid Function**\n",
        "       - **Step Function:** It produces discrete binary outputs based on whether the input is above or below a threshold. It's non-differentiable and mainly used in basic models.\n",
        "       - **Sigmoid Function:** It's a smooth S-shaped curve that maps inputs to values between 0 and 1. It's differentiable, allowing gradients to be propagated during training, and is used in many neural network architectures.\n",
        "\n",
        "    3. **Single Layer vs Multi-layer Perceptron**\n",
        "       - **Single Layer Perceptron:** It consists of only an input and an output layer. It can only learn linearly separable patterns and lacks the capacity to solve complex problems.\n",
        "       - **Multi-layer Perceptron:** It includes one or more hidden layers between the input and output layers. It can capture non-linear patterns and solve more intricate problems by introducing non-linearity through hidden layers."
      ],
      "metadata": {
        "id": "htAsN3vdFRwh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdGlLkv0FSde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}