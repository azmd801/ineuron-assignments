{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory%20/Assignment_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
        "\n",
        "**Answer:**\n",
        "Logistic Regression is generally preferable because it outputs probabilities for the outcomes, which can be more informative than the binary outputs of the Perceptron. Moreover, Logistic Regression is equipped with a log-likelihood loss function which is convex, facilitating the optimization process. To make a Perceptron equivalent to a Logistic Regression classifier, you can replace the step function with a logistic function (or sigmoid function) and train it using a method such as gradient descent to minimize the log-likelihood loss.\n",
        "\n",
        "### 2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
        "\n",
        "**Answer:**\n",
        "The logistic activation function was a key ingredient because it is a smooth, differentiable function, which allows for the use of gradient-based optimization methods such as gradient descent during training. Its S-shaped curve also helps to output probabilities, which is beneficial in classification tasks.\n",
        "\n",
        "### 3. Name three popular activation functions. Can you draw them?\n",
        "\n",
        "**Answer:**\n",
        "Three popular activation functions are:\n",
        "1. Sigmoid function\n",
        "2. Rectified Linear Unit (ReLU)\n",
        "3. Hyperbolic tangent (tanh)\n",
        "\n",
        "(To draw them, you can use a plotting library like matplotlib to plot their graphs in the next code cell.)\n",
        "\n",
        "### 4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
        "- What is the shape of the input matrix X?\n",
        "- What about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
        "- What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
        "- What is the shape of the network’s output matrix Y?\n",
        "- Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo, and bo.\n",
        "\n",
        "**Answer:**\n",
        "- The shape of the input matrix X is (n_samples, 10).\n",
        "- The shape of the hidden layer’s weight vector Wh is (10, 50) and the shape of its bias vector bh is (50,).\n",
        "- The shape of the output layer’s weight vector Wo is (50, 3) and its bias vector bo is (3,).\n",
        "- The shape of the network’s output matrix Y is (n_samples, 3).\n",
        "- The equation to compute the network's output matrix Y is given by:\n",
        "  \\[\n",
        "  Y = ReLU(ReLU(X \\cdot Wh + bh) \\cdot Wo + bo)\n",
        "  \\]\n",
        "\n",
        "### 5. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function?\n",
        "\n",
        "**Answer:**\n",
        "- For spam or ham classification, you would need one neuron in the output layer. The activation function used should be the sigmoid function to output probabilities.\n",
        "- For MNIST, you would need 10 neurons in the output layer, one for each digit (0-9). The activation function used should be the softmax function to output probabilities for each class.\n",
        "\n",
        "### 6. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
        "\n",
        "**Answer:**\n",
        "Backpropagation is an algorithm used in training neural networks, where gradients of the loss function with respect to the weights are computed for updating the weights. It works by applying the chain rule of calculus in a recursive manner from the output layer to the input layer. The difference between backpropagation and reverse-mode autodiff is that backpropagation is specifically tailored for the training of neural networks, whereas reverse-mode autodiff is a more general technique used for computing derivatives of functions, and can be applied in many different contexts, not limited to neural networks.\n",
        "\n",
        "### 7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "\n",
        "**Answer:**\n",
        "In an MLP, you can tweak several hyperparameters including:\n",
        "- Number of layers\n",
        "- Number of neurons in each layer\n",
        "- Activation functions used in each layer\n",
        "- Weight initialization strategy\n",
        "- Learning rate\n",
        "- Batch size\n",
        "- Regularization parameters (like dropout rate, L1/L2 regularization)\n",
        "\n",
        "If the MLP overfits the training data, you can try to solve the problem by:\n",
        "- Reducing the number of layers or the number of neurons in each layer to make the model less complex\n",
        "- Increasing the regularization parameters to reduce the complexity of the model\n",
        "- Using a different weight initialization strategy to help the model converge to a better solution\n",
        "- Adjusting the learning rate to help the model converge more smoothly\n",
        "- Using early stopping to prevent the model from continuing to learn once it starts overfitting"
      ],
      "metadata": {
        "id": "TRPBuCqCmu7E"
      },
      "id": "TRPBuCqCmu7E"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5lhVu1DmyLE"
      },
      "id": "b5lhVu1DmyLE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}