{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz90u4ER4Lu2YIn2EZ2Gwm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/ineuron-assignments/blob/main/DL%20theory/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is the function of a summation junction of a neuron? What is threshold activation function?**\n",
        "\n",
        "   The summation junction of a neuron computes the weighted sum of its inputs. This sum, along with a bias term, is then passed through the threshold activation function. The threshold activation function determines whether the neuron will produce an output signal based on whether the computed sum exceeds a certain threshold value.\n",
        "\n",
        "2. **What is a step function? What is the difference of step function with threshold function?**\n",
        "\n",
        "   A step function is a basic activation function that outputs a fixed value (usually 0 or 1) based on whether the input is above or below a threshold. The key difference between a step function and a threshold function is that the step function produces a discrete jump in output as soon as the threshold is crossed, while a threshold function may produce gradual changes in output after the threshold is exceeded.\n",
        "\n",
        "3. **Explain the McCulloch–Pitts model of neuron.**\n",
        "\n",
        "   The McCulloch–Pitts model is a simplified model of a biological neuron. It consists of multiple input connections with associated weights and a summation junction. The weighted inputs are summed, and if the sum surpasses a threshold, the neuron fires, producing an output signal. This model laid the foundation for neural network theory.\n",
        "\n",
        "4. **Explain the ADALINE network model.**\n",
        "\n",
        "   ADALINE (Adaptive Linear Neuron) is a single-layer neural network model. It uses a linear activation function and adapts its weights through a learning process to minimize the difference between its output and the desired output. ADALINE employs the Widrow-Hoff learning rule, which adjusts weights based on the error between the actual and desired outputs.\n",
        "\n",
        "5. **What is the constraint of a simple perceptron? Why it may fail with a real-world dataset?**\n",
        "\n",
        "   A simple perceptron can only learn linearly separable patterns. It fails with real-world datasets that are not linearly separable, meaning the classes cannot be separated by a straight line. Complex relationships or patterns in the data cannot be captured by a single perceptron.\n",
        "\n",
        "6. **What is the linearly inseparable problem? What is the role of the hidden layer?**\n",
        "\n",
        "   The linearly inseparable problem refers to datasets that cannot be separated by a linear decision boundary. The role of the hidden layer in a neural network is to introduce non-linearity. With hidden layers and non-linear activation functions, neural networks can capture complex patterns and solve linearly inseparable problems.\n",
        "\n",
        "7. **Explain XOR problem in case of a simple perceptron.**\n",
        "\n",
        "   The XOR problem is not solvable by a simple perceptron because it's a non-linearly separable problem. A perceptron can only create linear decision boundaries, and XOR requires a non-linear boundary. Therefore, a single perceptron cannot learn XOR successfully.\n",
        "\n",
        "8. **Design a multi-layer perceptron to implement A XOR B.**\n",
        "\n",
        "   A multi-layer perceptron with two input neurons, one hidden layer with two neurons using a non-linear activation function (e.g., sigmoid), and one output neuron can implement XOR. The hidden layer allows the network to capture the non-linear relationship required for XOR.\n",
        "\n",
        "9. **Explain the single-layer feed forward architecture of ANN.**\n",
        "\n",
        "   A single-layer feedforward neural network consists of an input layer and an output layer. The input layer passes data directly to the output layer, where each output neuron computes a weighted sum of inputs and applies an activation function. It's limited to linearly separable problems due to its lack of hidden layers.\n",
        "\n",
        "10. **Explain the competitive network architecture of ANN.**\n",
        "\n",
        "    Competitive networks, or self-organizing maps, are used for clustering and visualization. Neurons compete to be activated based on input similarity. The winning neuron and its neighbors adjust their weights to resemble the input, mapping high-dimensional data to a low-dimensional grid.\n",
        "\n",
        "11. **Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network.**\n",
        "\n",
        "    Backpropagation involves the following steps:\n",
        "    - Forward Pass: Compute outputs for each layer using current weights.\n",
        "    - Calculate Error: Measure the difference between predicted and actual outputs.\n",
        "    - Backward Pass: Propagate error backwards, adjusting weights using the gradient of the loss function with respect to weights.\n",
        "    - Update Weights: Use an optimization algorithm like gradient descent to update weights and minimize error.\n",
        "\n",
        "12. **What are the advantages and disadvantages of neural networks?**\n",
        "\n",
        "    **Advantages:** Neural networks can learn complex patterns, generalize from data, and solve non-linear problems. They're adaptable to various domains and can process large datasets.\n",
        "    \n",
        "    **Disadvantages:** They require large amounts of data and computational resources. Training can be slow, and they might overfit with insufficient data. Model interpretability can also be a challenge.\n",
        "\n",
        "13. **Write short notes on any two of the following:**\n",
        "\n",
        "    1. **Biological Neuron:** The basic building block of the nervous system, consisting of a cell body, dendrites, and an axon. Neurons transmit signals through synapses and use electrical impulses to communicate.\n",
        "    \n",
        "    2. **ReLU Function (Rectified Linear Activation):** A popular activation function that outputs the input directly if it's positive, otherwise outputs zero. It's widely used in deep learning due to its simplicity and ability to mitigate vanishing gradient problems."
      ],
      "metadata": {
        "id": "8xepcB56D8c9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldItswH-D9Tb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}